{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4629725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keyboard pyautogui pystray pillow opencv-python mediapipe scikit-learn tensorflow pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59d826fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "import keyboard\n",
    "import pygetwindow as gw\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f6f6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect samples\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "DATA_PATH = \"static_gestures\"\n",
    "NUM_SAMPLES = 200\n",
    "\n",
    "def collect_data(gesture):\n",
    "    os.makedirs(os.path.join(DATA_PATH, gesture), exist_ok=True)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    with mp_hands.Hands(min_detection_confidence=0.7) as hands:\n",
    "        count = 0\n",
    "        while count < NUM_SAMPLES:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            result = hands.process(rgb)\n",
    "\n",
    "            if result.multi_hand_landmarks:\n",
    "                hand_landmarks = result.multi_hand_landmarks[0]\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Extract 42 features (x, y for 21 landmarks)\n",
    "                landmarks = []\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    landmarks.extend([lm.x, lm.y])\n",
    "\n",
    "                np.save(os.path.join(DATA_PATH, gesture, f\"{count}.npy\"), np.array(landmarks))\n",
    "                count += 1\n",
    "                cv2.putText(frame, f\"Saved: {count}/{NUM_SAMPLES}\", (30, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow(\"Static Gesture Collection\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c313bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training\n",
    "\n",
    "def train_model():\n",
    "    X, y = [], []\n",
    "    gestures = os.listdir(DATA_PATH)\n",
    "\n",
    "    for gesture in gestures:\n",
    "        for file in os.listdir(os.path.join(DATA_PATH, gesture)):\n",
    "            landmarks = np.load(os.path.join(DATA_PATH, gesture, file))\n",
    "            X.append(landmarks)\n",
    "            y.append(gesture)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    y = to_categorical(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, activation=\"relu\", input_shape=(X.shape[1],)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(y.shape[1], activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))\n",
    "    model.save(\"static_gesture_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d24ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Music control\n",
    "\n",
    "GESTURES = [\"fist\", \"left\", \"open_music\", \"peace\", \"pinch\", \"right\", \"thumbs_up\", \"palm\"]\n",
    "gesture_actions = {\n",
    "    \"fist\": \"play_pause\",\n",
    "    \"right\": \"next_track\",\n",
    "    \"left\": \"previous_track\",\n",
    "    \"palm\": \"volume_up\",\n",
    "    \"pinch\": \"volume_down\",\n",
    "    \"peace\": \"stop\",\n",
    "    \"open_music\": \"open_music\"\n",
    "}\n",
    "\n",
    "last_triggered = {}\n",
    "COOLDOWN = 1.5\n",
    "YTM_COMMAND = \"https://music.youtube.com\"\n",
    "YTM_TITLE = \"YouTube Music\"\n",
    "\n",
    "def get_ytm_window():\n",
    "    wins = gw.getWindowsWithTitle(YTM_TITLE)\n",
    "    return wins[0] if wins else None\n",
    "\n",
    "def close_ytm():\n",
    "    win = get_ytm_window()\n",
    "    if win:\n",
    "        try:\n",
    "            win.close()\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(\"Error closing window:\", e)\n",
    "\n",
    "def control_music(action):\n",
    "    if action == \"play_pause\":\n",
    "        keyboard.send(\"play/pause media\")\n",
    "    elif action == \"next_track\":\n",
    "        keyboard.send(\"next track\")\n",
    "    elif action == \"previous_track\":\n",
    "        keyboard.send(\"previous track\")\n",
    "    elif action == \"volume_up\":\n",
    "        keyboard.send(\"volume up\")\n",
    "    elif action == \"volume_down\":\n",
    "        keyboard.send(\"volume down\")\n",
    "    elif action == \"stop\":\n",
    "        keyboard.send(\"stop media\")\n",
    "    elif action == \"open_music\":\n",
    "        win = get_ytm_window()\n",
    "        if not win:\n",
    "            print(\"Launching YouTube Music...\")\n",
    "            subprocess.Popen([\"explorer\", YTM_COMMAND])\n",
    "            time.sleep(10)\n",
    "            pyautogui.press(\"space\")  # auto-play\n",
    "        else:\n",
    "            if win.isActive:\n",
    "                print(\"Closing YouTube Music...\")\n",
    "                close_ytm()\n",
    "            else:\n",
    "                print(\"Switching focus...\")\n",
    "                win.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76285a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Program\n",
    "\n",
    "def run_gesture_control():\n",
    "    model = tf.keras.models.load_model(\"static_gesture_model.h5\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    with mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            result = hands.process(rgb)\n",
    "\n",
    "            if result.multi_hand_landmarks:\n",
    "                for hand_landmarks, hand_info in zip(result.multi_hand_landmarks, result.multi_handedness):\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    landmarks = []\n",
    "                    for lm in hand_landmarks.landmark:\n",
    "                        landmarks.extend([lm.x, lm.y])\n",
    "\n",
    "                    prediction = model.predict(np.expand_dims(landmarks, axis=0), verbose=0)[0]\n",
    "                    pred_class = GESTURES[np.argmax(prediction)]\n",
    "                    confidence = np.max(prediction)\n",
    "\n",
    "                    if confidence > 0.8:\n",
    "                        h, w, _ = frame.shape\n",
    "                        xs = [int(lm.x * w) for lm in hand_landmarks.landmark]\n",
    "                        ys = [int(lm.y * h) for lm in hand_landmarks.landmark]\n",
    "                        x_min, y_min = min(xs), min(ys)\n",
    "\n",
    "                        label = f\"{hand_info.classification[0].label}: {pred_class} ({confidence:.2f})\"\n",
    "                        cv2.putText(frame, label, (x_min, y_min - 10),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "                        action = gesture_actions.get(pred_class)\n",
    "                        if action:\n",
    "                            now = time.time()\n",
    "                            if now - last_triggered.get(action, 0) > COOLDOWN:\n",
    "                                control_music(action)\n",
    "                                last_triggered[action] = now\n",
    "\n",
    "            cv2.imshow(\"Music Control via Hand Gestures\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab752fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f8270",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9be685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching YouTube Music...\n"
     ]
    }
   ],
   "source": [
    "run_gesture_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa93c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.11 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
